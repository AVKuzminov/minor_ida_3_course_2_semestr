{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\akrazer\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: boto3 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: requests in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: bz2file in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: botocore<1.9.0,>=1.8.43 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from botocore<1.9.0,>=1.8.43->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from botocore<1.9.0,>=1.8.43->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages\n",
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.3.1.tar.gz\n",
      "Requirement already satisfied: matplotlib in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from wordcloud)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from wordcloud)\n",
      "Requirement already satisfied: pillow in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from wordcloud)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: pytz in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: olefile in c:\\users\\akrazer\\anaconda3\\lib\\site-packages (from pillow->wordcloud)\n",
      "Building wheels for collected packages: wordcloud\n",
      "  Running setup.py bdist_wheel for wordcloud: started\n",
      "  Running setup.py bdist_wheel for wordcloud: finished with status 'error'\n",
      "  Complete output from command C:\\Users\\akrazer\\Anaconda3\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\akrazer\\\\AppData\\\\Local\\\\Temp\\\\pip-build-s2i8soug\\\\wordcloud\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d C:\\Users\\akrazer\\AppData\\Local\\Temp\\tmpzi7o5i7jpip-wheel- --python-tag cp36:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.6\n",
      "  creating build\\lib.win-amd64-3.6\\wordcloud\n",
      "  copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "  copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "  copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "  copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "  copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "  copying wordcloud\\stopwords -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "  copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "  running build_ext\n",
      "  building 'wordcloud.query_integral_image' extension\n",
      "  error: [WinError 3] Системе не удается найти указанный путь: 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\PlatformSDK\\\\lib'\n",
      "  \n",
      "  ----------------------------------------\n",
      "  Running setup.py clean for wordcloud\n",
      "Failed to build wordcloud\n",
      "Installing collected packages: wordcloud\n",
      "  Running setup.py install for wordcloud: started\n",
      "    Running setup.py install for wordcloud: finished with status 'error'\n",
      "    Complete output from command C:\\Users\\akrazer\\Anaconda3\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\akrazer\\\\AppData\\\\Local\\\\Temp\\\\pip-build-s2i8soug\\\\wordcloud\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record C:\\Users\\akrazer\\AppData\\Local\\Temp\\pip-4xs7gqel-record\\install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.6\n",
      "    creating build\\lib.win-amd64-3.6\\wordcloud\n",
      "    copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "    copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "    copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "    copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "    copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "    copying wordcloud\\stopwords -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "    copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.6\\wordcloud\n",
      "    running build_ext\n",
      "    building 'wordcloud.query_integral_image' extension\n",
      "    error: [WinError 3] Системе не удается найти указанный путь: 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\PlatformSDK\\\\lib'\n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Failed building wheel for wordcloud\n",
      "Command \"C:\\Users\\akrazer\\Anaconda3\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\akrazer\\\\AppData\\\\Local\\\\Temp\\\\pip-build-s2i8soug\\\\wordcloud\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record C:\\Users\\akrazer\\AppData\\Local\\Temp\\pip-4xs7gqel-record\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\akrazer\\AppData\\Local\\Temp\\pip-build-s2i8soug\\wordcloud\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.19.5-py2.py3-none-any.whl\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.19.5\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim\n",
    "! pip install beautifulsoup4\n",
    "! pip install wordcloud\n",
    "! pip install tqdm\n",
    "\n",
    "# или используем https://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгружаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0,\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['review'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что в данных присутствуют элементы html.\n",
    "\n",
    "- избавимся от элементов html  с помощью модуля BeautifulSoup: \n",
    "    http://nullege.com/codes/search/bs4.BeautifulSoup.getText\n",
    "- оставим только буквы\n",
    "- приведем все к нижнему регистру\n",
    "- уберем стоп слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-01f6eefc7218>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;31m# Packages which can be lazily imported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\stem\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misri\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mISRIStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrslp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRSLPStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\stem\\snowball.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mporter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msuffix_replace\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyCorpusLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m abc = LazyCorpusLoader(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \"\"\"\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaintext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mPlaintextCorpusReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCorpusReader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \"\"\"\n\u001b[0;32m     25\u001b[0m     \u001b[0mReader\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcorpora\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mconsist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mplaintext\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mParagraphs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001b[0m in \u001b[0;36mPlaintextCorpusReader\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m     def __init__(self, root, fileids,\n\u001b[0;32m     41\u001b[0m                  \u001b[0mword_tokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWordPunctTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                  sent_tokenizer=nltk.data.LazyLoader(\n\u001b[0m\u001b[0;32m     43\u001b[0m                      'tokenizers/punkt/english.pickle'),\n\u001b[0;32m     44\u001b[0m                  \u001b[0mpara_block_reader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_blankline_block\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    # your code is hear\n",
    "    # 1. bs\n",
    "    bs = BeautifulSoup(raw_review)\n",
    "    review_text = bs.get_text()\n",
    "    # 2. regular expression\n",
    "    rege = re.compile('\\w+')\n",
    "    letters_only = rege.findall(review_text)\n",
    "    # 3. to lower\n",
    "    words = [word.lower() for word in letters_only]\n",
    "    # 4. drop stopwords\n",
    "    sw = stopwords.words('english')\n",
    "    words = [w for w in words if w.lower() not in sw]    \n",
    "    return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\ProgramData\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stuff',\n",
       " 'going',\n",
       " 'moment',\n",
       " 'mj',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'odd',\n",
       " 'documentary',\n",
       " 'watched',\n",
       " 'wiz',\n",
       " 'watched',\n",
       " 'moonwalker',\n",
       " 'maybe',\n",
       " 'want',\n",
       " 'get',\n",
       " 'certain',\n",
       " 'insight',\n",
       " 'guy',\n",
       " 'thought',\n",
       " 'really',\n",
       " 'cool',\n",
       " 'eighties',\n",
       " 'maybe',\n",
       " 'make',\n",
       " 'mind',\n",
       " 'whether',\n",
       " 'guilty',\n",
       " 'innocent',\n",
       " 'moonwalker',\n",
       " 'part',\n",
       " 'biography',\n",
       " 'part',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'remember',\n",
       " 'going',\n",
       " 'see',\n",
       " 'cinema',\n",
       " 'originally',\n",
       " 'released',\n",
       " 'subtle',\n",
       " 'messages',\n",
       " 'mj',\n",
       " 'feeling',\n",
       " 'towards',\n",
       " 'press',\n",
       " 'also',\n",
       " 'obvious',\n",
       " 'message',\n",
       " 'drugs',\n",
       " 'bad',\n",
       " 'kay',\n",
       " 'visually',\n",
       " 'impressive',\n",
       " 'course',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'unless',\n",
       " 'remotely',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'anyway',\n",
       " 'going',\n",
       " 'hate',\n",
       " 'find',\n",
       " 'boring',\n",
       " 'may',\n",
       " 'call',\n",
       " 'mj',\n",
       " 'egotist',\n",
       " 'consenting',\n",
       " 'making',\n",
       " 'movie',\n",
       " 'mj',\n",
       " 'fans',\n",
       " 'would',\n",
       " 'say',\n",
       " 'made',\n",
       " 'fans',\n",
       " 'true',\n",
       " 'really',\n",
       " 'nice',\n",
       " 'actual',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'bit',\n",
       " 'finally',\n",
       " 'starts',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'excluding',\n",
       " 'smooth',\n",
       " 'criminal',\n",
       " 'sequence',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'convincing',\n",
       " 'psychopathic',\n",
       " 'powerful',\n",
       " 'drug',\n",
       " 'lord',\n",
       " 'wants',\n",
       " 'mj',\n",
       " 'dead',\n",
       " 'bad',\n",
       " 'beyond',\n",
       " 'mj',\n",
       " 'overheard',\n",
       " 'plans',\n",
       " 'nah',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'character',\n",
       " 'ranted',\n",
       " 'wanted',\n",
       " 'people',\n",
       " 'know',\n",
       " 'supplying',\n",
       " 'drugs',\n",
       " 'etc',\n",
       " 'dunno',\n",
       " 'maybe',\n",
       " 'hates',\n",
       " 'mj',\n",
       " 'music',\n",
       " 'lots',\n",
       " 'cool',\n",
       " 'things',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'turning',\n",
       " 'car',\n",
       " 'robot',\n",
       " 'whole',\n",
       " 'speed',\n",
       " 'demon',\n",
       " 'sequence',\n",
       " 'also',\n",
       " 'director',\n",
       " 'must',\n",
       " 'patience',\n",
       " 'saint',\n",
       " 'came',\n",
       " 'filming',\n",
       " 'kiddy',\n",
       " 'bad',\n",
       " 'sequence',\n",
       " 'usually',\n",
       " 'directors',\n",
       " 'hate',\n",
       " 'working',\n",
       " 'one',\n",
       " 'kid',\n",
       " 'let',\n",
       " 'alone',\n",
       " 'whole',\n",
       " 'bunch',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'dance',\n",
       " 'scene',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'movie',\n",
       " 'people',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'one',\n",
       " 'level',\n",
       " 'another',\n",
       " 'think',\n",
       " 'people',\n",
       " 'stay',\n",
       " 'away',\n",
       " 'try',\n",
       " 'give',\n",
       " 'wholesome',\n",
       " 'message',\n",
       " 'ironically',\n",
       " 'mj',\n",
       " 'bestest',\n",
       " 'buddy',\n",
       " 'movie',\n",
       " 'girl',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'truly',\n",
       " 'one',\n",
       " 'talented',\n",
       " 'people',\n",
       " 'ever',\n",
       " 'grace',\n",
       " 'planet',\n",
       " 'guilty',\n",
       " 'well',\n",
       " 'attention',\n",
       " 'gave',\n",
       " 'subject',\n",
       " 'hmmm',\n",
       " 'well',\n",
       " 'know',\n",
       " 'people',\n",
       " 'different',\n",
       " 'behind',\n",
       " 'closed',\n",
       " 'doors',\n",
       " 'know',\n",
       " 'fact',\n",
       " 'either',\n",
       " 'extremely',\n",
       " 'nice',\n",
       " 'stupid',\n",
       " 'guy',\n",
       " 'one',\n",
       " 'sickest',\n",
       " 'liars',\n",
       " 'hope',\n",
       " 'latter']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train['review'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примените реализованную функцию к каждому предожению train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/1000 [00:00<?, ?it/s]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\ProgramData\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:02<00:00, 362.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#num_reviews = train[\"review\"].size \n",
    "num_reviews = 1000\n",
    "\n",
    "clean_train_reviews = []\n",
    "\n",
    "for i in tqdm(range(num_reviews)):\n",
    "    clean_train_reviews.append(review_to_words(train[\"review\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# подаем в обучение clean_train_reviews\n",
    "model = word2vec.Word2Vec(clean_train_reviews, workers=num_workers, \n",
    "                          size=num_features, min_count = min_word_count, \n",
    "                          window = context, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# модель можно сохранить\n",
    "# model_name = ''\n",
    "# model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно воспользjваться функциями поиска похожих / непохожих слов на данное\n",
    "- model.wv.most_similar\n",
    "- model.wv.doesnt_match\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# здесь можно посмотреть все слова, к которым мы имеем вектора\n",
    "#model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('father', 0.9997919797897339),\n",
       " ('next', 0.9997912645339966),\n",
       " ('human', 0.9997851848602295),\n",
       " ('friend', 0.9997850656509399),\n",
       " ('called', 0.9997849464416504),\n",
       " ('course', 0.9997843503952026),\n",
       " ('son', 0.9997842907905579),\n",
       " ('name', 0.9997837543487549),\n",
       " ('becomes', 0.9997789859771729),\n",
       " ('woman', 0.9997783899307251)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('mother')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'child'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man woman child\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earth'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match([\"sun\", \"moon\", \"planet\", \"earth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('next', 0.9998335838317871),\n",
       " ('set', 0.9998206496238708),\n",
       " ('tries', 0.9998139142990112),\n",
       " ('children', 0.9998130202293396),\n",
       " ('looking', 0.9998107552528381),\n",
       " ('goes', 0.9998093843460083),\n",
       " ('lead', 0.9998011589050293),\n",
       " ('supposed', 0.9997996687889099),\n",
       " ('given', 0.9997984170913696),\n",
       " ('always', 0.9997984170913696)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('early', 0.9997045397758484),\n",
       " ('based', 0.9997020363807678),\n",
       " ('genre', 0.9997010231018066),\n",
       " ('bit', 0.9996837377548218),\n",
       " ('sense', 0.9996798038482666),\n",
       " ('pretty', 0.9996776580810547),\n",
       " ('fans', 0.9996731281280518),\n",
       " ('kind', 0.9996672868728638),\n",
       " ('british', 0.9996665716171265),\n",
       " ('dialogue', 0.9996662139892578)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"horror\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь выделите top 100 самых частотных слов и получите для них только что обученные вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\ProgramData\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stuff',\n",
       " 'going',\n",
       " 'moment',\n",
       " 'mj',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'odd',\n",
       " 'documentary',\n",
       " 'watched',\n",
       " 'wiz',\n",
       " 'watched',\n",
       " 'moonwalker',\n",
       " 'maybe',\n",
       " 'want',\n",
       " 'get',\n",
       " 'certain',\n",
       " 'insight',\n",
       " 'guy',\n",
       " 'thought',\n",
       " 'really',\n",
       " 'cool',\n",
       " 'eighties',\n",
       " 'maybe',\n",
       " 'make',\n",
       " 'mind',\n",
       " 'whether',\n",
       " 'guilty',\n",
       " 'innocent',\n",
       " 'moonwalker',\n",
       " 'part',\n",
       " 'biography',\n",
       " 'part',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'remember',\n",
       " 'going',\n",
       " 'see',\n",
       " 'cinema',\n",
       " 'originally',\n",
       " 'released',\n",
       " 'subtle',\n",
       " 'messages',\n",
       " 'mj',\n",
       " 'feeling',\n",
       " 'towards',\n",
       " 'press',\n",
       " 'also',\n",
       " 'obvious',\n",
       " 'message',\n",
       " 'drugs',\n",
       " 'bad',\n",
       " 'kay',\n",
       " 'visually',\n",
       " 'impressive',\n",
       " 'course',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'unless',\n",
       " 'remotely',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'anyway',\n",
       " 'going',\n",
       " 'hate',\n",
       " 'find',\n",
       " 'boring',\n",
       " 'may',\n",
       " 'call',\n",
       " 'mj',\n",
       " 'egotist',\n",
       " 'consenting',\n",
       " 'making',\n",
       " 'movie',\n",
       " 'mj',\n",
       " 'fans',\n",
       " 'would',\n",
       " 'say',\n",
       " 'made',\n",
       " 'fans',\n",
       " 'true',\n",
       " 'really',\n",
       " 'nice',\n",
       " 'actual',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'bit',\n",
       " 'finally',\n",
       " 'starts',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'excluding',\n",
       " 'smooth',\n",
       " 'criminal',\n",
       " 'sequence',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'convincing',\n",
       " 'psychopathic',\n",
       " 'powerful',\n",
       " 'drug',\n",
       " 'lord',\n",
       " 'wants',\n",
       " 'mj',\n",
       " 'dead',\n",
       " 'bad',\n",
       " 'beyond',\n",
       " 'mj',\n",
       " 'overheard',\n",
       " 'plans',\n",
       " 'nah',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'character',\n",
       " 'ranted',\n",
       " 'wanted',\n",
       " 'people',\n",
       " 'know',\n",
       " 'supplying',\n",
       " 'drugs',\n",
       " 'etc',\n",
       " 'dunno',\n",
       " 'maybe',\n",
       " 'hates',\n",
       " 'mj',\n",
       " 'music',\n",
       " 'lots',\n",
       " 'cool',\n",
       " 'things',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'turning',\n",
       " 'car',\n",
       " 'robot',\n",
       " 'whole',\n",
       " 'speed',\n",
       " 'demon',\n",
       " 'sequence',\n",
       " 'also',\n",
       " 'director',\n",
       " 'must',\n",
       " 'patience',\n",
       " 'saint',\n",
       " 'came',\n",
       " 'filming',\n",
       " 'kiddy',\n",
       " 'bad',\n",
       " 'sequence',\n",
       " 'usually',\n",
       " 'directors',\n",
       " 'hate',\n",
       " 'working',\n",
       " 'one',\n",
       " 'kid',\n",
       " 'let',\n",
       " 'alone',\n",
       " 'whole',\n",
       " 'bunch',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'dance',\n",
       " 'scene',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'movie',\n",
       " 'people',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'one',\n",
       " 'level',\n",
       " 'another',\n",
       " 'think',\n",
       " 'people',\n",
       " 'stay',\n",
       " 'away',\n",
       " 'try',\n",
       " 'give',\n",
       " 'wholesome',\n",
       " 'message',\n",
       " 'ironically',\n",
       " 'mj',\n",
       " 'bestest',\n",
       " 'buddy',\n",
       " 'movie',\n",
       " 'girl',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'truly',\n",
       " 'one',\n",
       " 'talented',\n",
       " 'people',\n",
       " 'ever',\n",
       " 'grace',\n",
       " 'planet',\n",
       " 'guilty',\n",
       " 'well',\n",
       " 'attention',\n",
       " 'gave',\n",
       " 'subject',\n",
       " 'hmmm',\n",
       " 'well',\n",
       " 'know',\n",
       " 'people',\n",
       " 'different',\n",
       " 'behind',\n",
       " 'closed',\n",
       " 'doors',\n",
       " 'know',\n",
       " 'fact',\n",
       " 'either',\n",
       " 'extremely',\n",
       " 'nice',\n",
       " 'stupid',\n",
       " 'guy',\n",
       " 'one',\n",
       " 'sickest',\n",
       " 'liars',\n",
       " 'hope',\n",
       " 'latter']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train['review'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9a4ff5aec49b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreview_to_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;31m###########################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChunkParserI\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m from nltk.chunk.util import (ChunkScore, accuracy, tagstr2tree, conllstr2tree,\n\u001b[0;32m    159\u001b[0m                              \u001b[0mconlltags2tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree2conllstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m##//////////////////////////////////////////////////////\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParserI\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChunkScore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\parse\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviterbi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mViterbiParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdependencygraph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDependencyGraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m from nltk.parse.projectivedependencyparser import (ProjectiveDependencyParser,\n\u001b[0m\u001b[0;32m     73\u001b[0m                                                    ProbabilisticProjectiveDependencyParser)\n\u001b[0;32m     74\u001b[0m from nltk.parse.nonprojectivedependencyparser import (NonprojectiveDependencyParser,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fd = FreqDist()\n",
    "for i in range(len(train[\"review\"])):\n",
    "    text = review_to_words(train[\"review\"].iloc[i])\n",
    "    for j in range(len(text)):\n",
    "        if j in fd:\n",
    "            fd[text[j]] += 1\n",
    "        else:\n",
    "            fd[text[j]] = 0\n",
    "fd\n",
    "\n",
    "# your code is hear\n",
    "\n",
    "#top_words = # your code is hear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words_vec = # your code is hear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем вектора. Для этого воспользуемся методом снижения размерности t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "top_words_tsne = # your code is hear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(top_words_vec.shape)\n",
    "print(top_words_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for value in top_words_tsne:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "\n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(100):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(top_words[i],\n",
    "                 xy=(x[i], y[i]),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нарисуйте облако слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import *\n",
    "word_freq = [i for i in fd.most_common(100)]\n",
    "wd = WordCloud(background_color = 'white')\n",
    "wd.generate_from_frequencies(dict(word_freq))\n",
    "plt.figure()\n",
    "plt.imshow(wd, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
